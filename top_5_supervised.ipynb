{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THXtBeuSMrGA"
      },
      "source": [
        "1. **Goal 1**:Train a fully supervised baseline neural network. For this goal, you will use a simple ResNet (provided below) and the DataLoader class you have been implementing to train a neural network to predict what plant species is present in a given image in a fully supervised way.\n",
        "> * As a reminder, [fully supervised learning](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d) involves training a model to predict the output `Y` from input `X`. For your project, `X` is an image of a plant\n",
        "\n",
        "> >>![plant image](https://inaturalist-open-data.s3.amazonaws.com/photos/126743755/medium.jpg)\n",
        "\n",
        "> * While `Y` is the name of the corresponding plant (i.e.: *Hesperis matronalis* or Dame's Rocket as it's more commonly known).\n",
        "\n",
        "2. **Goal 2**: Train a simple triplet loss contrastive self-supervised baseline. This form of training has two training steps, unlike the classic fully supervised version.\n",
        "> In the first step for self-supervised learning, we throw out the label `Y` for the first part of training, instead opting to use three images from our dataset, `X`, `f(X)`, and `~X`.\n",
        "> In this case, `X` is an image, like above, `f(X)` is some form of *augmentation* of the image, like a flip or a crop\n",
        "\n",
        ">> ![da](https://editor.analyticsvidhya.com/uploads/84333flip.jpg)\n",
        ">\n",
        "> Finally, `~X` is typically another image randomly sampled from the dataset, like:\n",
        "> >>![plant image](https://inaturalist-open-data.s3.amazonaws.com/photos/25176952/medium.jpeg)\n",
        "\n",
        "> In the second step of self-supervised learning, we take our trained neural network and *freeze the weights* (meaning, those real-valued parameters will no longer be updated by gradient descent during training). We then swap the last layer with a new one that corresponds to our classes from `Y` (in this case, that correspond to all the plant species we're trying identify). We then train our neural network for an additional amount of time, only updating this new output layer (this is called **feature extraction** which is explained along with other forms of transfer learning [here](https://stats.stackexchange.com/questions/255364/fine-tuning-vs-joint-training-vs-feature-extraction)).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YeAJwzfUyPz",
        "outputId": "cdd94def-3cdf-4e07-b7dd-847262b85110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N139gkD-WLyt"
      },
      "outputs": [],
      "source": [
        "!unzip \"gdrive/Shareddrives/197/project_1_dataset.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sfd7e-5NGgjF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "label_dict = {}\n",
        "f_train = pd.read_csv('/content/project_1/train/observations.csv')\n",
        "i = 0\n",
        "for row in range(f_train.shape[0]):\n",
        "  specie = f_train.iloc[row,4]\n",
        "  if (specie not in label_dict):\n",
        "    label_dict[specie] = i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDImWCvmgCEq"
      },
      "outputs": [],
      "source": [
        "#label_dict = {}\n",
        "#f_train = pd.read_csv('/content/project_1/train/observations.csv')\n",
        "#for row in f_train.iterrows():\n",
        "#  specie = row['taxon_id'] # or row.taxon_id #  f_train.iloc[row,4]\n",
        "#  if (specie not in label_dict):\n",
        "#    label_dict[specie] = i\n",
        "#    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMtEAVEVatVo"
      },
      "outputs": [],
      "source": [
        "# machine learning packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# miscellaneous packages\n",
        "import json\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm, trange\n",
        "from types import SimpleNamespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQbRL1-0VF1v"
      },
      "outputs": [],
      "source": [
        "# Neural network implementation for ResNet 18\n",
        "# for a visual walkthrough of this neural network, see: https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8\n",
        "# code from: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
        "class BasicBlock(nn.Module):\n",
        "  # the basic block implements a basic residual block\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            inplanes: int,\n",
        "            planes: int,\n",
        "            stride: int = 1,\n",
        "            downsample = None,\n",
        "            groups: int = 1,\n",
        "            base_width = 64,\n",
        "            dilation = 1,\n",
        "            norm_layer = None):\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "      # the actual residual part!\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXWCaRt4Z0N3"
      },
      "outputs": [],
      "source": [
        "# sets up the convolutional layers for us all nice\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bjVHHhS-zZ"
      },
      "outputs": [],
      "source": [
        "# actual ResNet class implementation\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            block, # what kind of block to use. Small ResNets only use basic blocks\n",
        "            layers, # number of BasicBlocks to make in each layer of ResNet\n",
        "            num_classes: int = 64, # number of species\n",
        "            groups: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "       # magic numbers to match the matrix sizes for matrix multiplication\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        # for ResNet18 and BasicBlock, the group and base_width must always be these vals\n",
        "        self.groups = 1\n",
        "        self.base_width = 64\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        # at some point, you may want to do the math so that your feature\n",
        "        # extraction layer has > 512 features in it. In the past, I've found\n",
        "        # 2048 to be a better final layer size.\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "      # initialize the model weights to be small real values drawn from a Z normal distribution\n",
        "      # turns out starting all the parameter values at 0 can lead to either untrainable or unstable NNs\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    # sets up all the gross math so the weight matrices line up correctly\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block,\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCMqBkF2RLGD"
      },
      "outputs": [],
      "source": [
        "# Dataset and dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "# TODO: copy in your dataset and dataloader classes here\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class WildFlowerDataset(Dataset):\n",
        "    def __init__(self, data_frame, img_dir, transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(csv)\n",
        "        self.img_labels = data_frame\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # crop to square 150x150 pixels and pad images that are less than that\n",
        "        photo_id_train = self.img_labels.iloc[idx,13]\n",
        "        folder_num_train = photo_id_train // 1000000\n",
        "\n",
        "        img_path = self.img_dir + \"/\" + str(folder_num_train) + \"/\" + str(photo_id_train) + \".png\"\n",
        "        img_og = Image.open(img_path)\n",
        "        #ensure image is png\n",
        "        #background = Image.new('RGBA', img_og.size, (255, 255, 255))\n",
        "        #alpha_composite = Image.alpha_composite(background, img_og)\n",
        "        #alpha_composite.save(img_og, 'png', quality=100)\n",
        "        tensor_img = transforms.ToTensor()(img_og)\n",
        "        tensor_img = tensor_img[:3, : , : ]\n",
        "\n",
        "        if tensor_img.shape[0] < 3:  # Check the number of channels\n",
        "            tensor_img = tensor_img.repeat(3, 1, 1)\n",
        "\n",
        "        label = label_dict[self.img_labels.iloc[idx, 4]]\n",
        "\n",
        "        if self.transform:\n",
        "            tensor_img = self.transform(tensor_img)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        crop = transforms.CenterCrop((150,150))\n",
        "        image = crop(tensor_img)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZpInh8LEyUL"
      },
      "outputs": [],
      "source": [
        "# Train loop\n",
        "def train(args, model, device, train_loader, optimizer, epoch, logger, count, SummaryWriter):\n",
        "    writer = SummaryWriter\n",
        "\n",
        "\n",
        "  # have to remember to put your model in training mode!\n",
        "    model.train()\n",
        "    # logs loss per-batch to the console\n",
        "    # stats is a range object from zero to len(train_loader)\n",
        "    stats = trange(len(train_loader))\n",
        "    indexer = epoch * len(train_loader)\n",
        "    for batch_idx, (data, target)  in zip(stats, train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # standard softmax cross-entropy loss\n",
        "        # explainer here: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # save train loss to tqdm bar\n",
        "        stats.set_description(f'epoch {epoch}')\n",
        "        stats.set_postfix(loss=loss.item())\n",
        "        logger[batch_idx+indexer] = loss.item()\n",
        "\n",
        "        #added for logging\n",
        "        writer.add_scalar('Train/Loss', loss.item(), count)\n",
        "        count+=1\n",
        "    stats.close()\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wDa2ArlRfrN"
      },
      "outputs": [],
      "source": [
        "# Test loop\n",
        "def test(model, device, test_loader, epoch, logger, count, SummaryWriter):\n",
        "  #logging\n",
        "    writer = SummaryWriter\n",
        "  # need to put into eval() mode to ensure model outputs are deterministic\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    top5correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader, total=len(test_loader), desc=f'testing epoch {epoch}'):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            #logging for test loss\n",
        "            writer.add_scalar('Test/Loss', (F.cross_entropy(output, target, reduction='sum').item()), count)\n",
        "            count+=1\n",
        "            # see if the highest predicted class was the right class\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "            top5, indices = output.topk(5, dim=1)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            for c in range(indices.size(1)):\n",
        "              c_tens = indices[:,c]\n",
        "              top5correct += c_tens.eq(target.view_as(c_tens)).sum().item()\n",
        "    # get average test loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'test set avg loss: {round(test_loss, 6)} Acc: {correct}/{len(test_loader.dataset)}: {100*round(correct / len(test_loader.dataset), 4)}%')\n",
        "    logger['loss'][epoch] = round(test_loss, 6)\n",
        "    logger['1accuracy'][epoch] = 100*round(correct / len(test_loader.dataset), 6)\n",
        "    print(f'test set avg loss top-5: {round(test_loss, 6)} Acc: {top5correct}/{len(test_loader.dataset)}: {100*round(top5correct / len(test_loader.dataset), 4)}%')\n",
        "    logger['5accuracy'][epoch] = 100*round(top5correct / len(test_loader.dataset), 6)\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GCJw8owcVR7"
      },
      "outputs": [],
      "source": [
        "# set up hyperparameters\n",
        "hyperparams = {\n",
        "    'batch_size' : 64, # 64\n",
        "    'test_batch_size' : 1000, #1000\n",
        "    'epochs' : 12,\n",
        "    'lr' : 0.001,\n",
        "    'gamma' : 0.7,\n",
        "    'seed' : 1,\n",
        "}\n",
        "hyperparams = SimpleNamespace(**hyperparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVmo8rZScdTU"
      },
      "outputs": [],
      "source": [
        "# TODO: instantiate your dataset and loader here\n",
        "import torchvision.transforms as transforms\n",
        "f_test = pd.read_csv('/content/project_1/test/observations.csv')\n",
        "f_val = pd.read_csv('/content/project_1/validation/observations.csv')\n",
        "f_test = f_test.loc[f_test['taxon_id'].isin(label_dict)]\n",
        "train_dset = WildFlowerDataset(f_train,'/content/project_1/train/images' , transform=None, target_transform=None)\n",
        "test_dset = WildFlowerDataset(f_test,'/content/project_1/test/images' , transform=None, target_transform=None)\n",
        "val_dset = WildFlowerDataset(f_val,'/content/project_1/validation/images' , transform=None, target_transform=None)\n",
        "train_loader = DataLoader(train_dset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L8DLcGhcrr8"
      },
      "outputs": [],
      "source": [
        "# this loads in a randomly initialized ResNet18\n",
        "# if you want to try using imagenet-pretrained weights at some point,\n",
        "# check out this article: https://pytorch.org/vision/master/models.html\n",
        "model = ResNet18(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# set up optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n",
        "\n",
        "# set up cuda devices\n",
        "device = torch.device(\"cuda\")\n",
        "torch.manual_seed(hyperparams.seed);\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyVIJPEcp37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf3e238-673f-43ba-918b-cad2ec7ac65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0: 100%|██████████| 1221/1221 [03:01<00:00,  6.72it/s, loss=1.91]\n",
            "testing epoch 0: 100%|██████████| 921/921 [02:03<00:00,  7.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 2.363505 Acc: 20857/58894: 35.410000000000004%\n",
            "test set avg loss top-5: 2.363505 Acc: 40161/58894: 68.19%\n",
            "starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1: 100%|██████████| 1221/1221 [03:00<00:00,  6.76it/s, loss=1.32]\n",
            "testing epoch 1: 100%|██████████| 921/921 [02:02<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 2.112585 Acc: 23978/58894: 40.71%\n",
            "test set avg loss top-5: 2.112585 Acc: 44955/58894: 76.33%\n",
            "starting epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 2: 100%|██████████| 1221/1221 [02:59<00:00,  6.82it/s, loss=1.8]\n",
            "testing epoch 2: 100%|██████████| 921/921 [02:02<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.565945 Acc: 32164/58894: 54.61%\n",
            "test set avg loss top-5: 1.565945 Acc: 49651/58894: 84.31%\n",
            "starting epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 3: 100%|██████████| 1221/1221 [02:58<00:00,  6.83it/s, loss=1.43]\n",
            "testing epoch 3: 100%|██████████| 921/921 [02:02<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.657959 Acc: 32057/58894: 54.43%\n",
            "test set avg loss top-5: 1.657959 Acc: 48756/58894: 82.78999999999999%\n",
            "starting epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 4: 100%|██████████| 1221/1221 [02:59<00:00,  6.82it/s, loss=0.851]\n",
            "testing epoch 4: 100%|██████████| 921/921 [02:03<00:00,  7.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.919742 Acc: 30581/58894: 51.93%\n",
            "test set avg loss top-5: 1.919742 Acc: 48428/58894: 82.23%\n",
            "starting epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 5: 100%|██████████| 1221/1221 [02:59<00:00,  6.81it/s, loss=1.15]\n",
            "testing epoch 5: 100%|██████████| 921/921 [02:03<00:00,  7.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.587501 Acc: 34718/58894: 58.95%\n",
            "test set avg loss top-5: 1.587501 Acc: 50974/58894: 86.55000000000001%\n",
            "starting epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 6: 100%|██████████| 1221/1221 [02:59<00:00,  6.80it/s, loss=0.494]\n",
            "testing epoch 6: 100%|██████████| 921/921 [02:02<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.640724 Acc: 35143/58894: 59.67%\n",
            "test set avg loss top-5: 1.640724 Acc: 50748/58894: 86.17%\n",
            "starting epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 7: 100%|██████████| 1221/1221 [02:59<00:00,  6.81it/s, loss=0.717]\n",
            "testing epoch 7: 100%|██████████| 921/921 [02:02<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 2.024268 Acc: 32559/58894: 55.279999999999994%\n",
            "test set avg loss top-5: 2.024268 Acc: 48873/58894: 82.98%\n",
            "starting epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 8: 100%|██████████| 1221/1221 [02:59<00:00,  6.81it/s, loss=0.432]\n",
            "testing epoch 8: 100%|██████████| 921/921 [02:02<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.654728 Acc: 36849/58894: 62.57%\n",
            "test set avg loss top-5: 1.654728 Acc: 51962/58894: 88.23%\n",
            "starting epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 9: 100%|██████████| 1221/1221 [02:59<00:00,  6.82it/s, loss=0.197]\n",
            "testing epoch 9: 100%|██████████| 921/921 [02:02<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 1.781952 Acc: 37806/58894: 64.19%\n",
            "test set avg loss top-5: 1.781952 Acc: 52206/58894: 88.64%\n",
            "starting epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 10: 100%|██████████| 1221/1221 [02:58<00:00,  6.84it/s, loss=0.182]\n",
            "testing epoch 10: 100%|██████████| 921/921 [02:02<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 2.236243 Acc: 34316/58894: 58.269999999999996%\n",
            "test set avg loss top-5: 2.236243 Acc: 49925/58894: 84.77%\n",
            "starting epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 11: 100%|██████████| 1221/1221 [02:59<00:00,  6.79it/s, loss=0.565]\n",
            "testing epoch 11: 100%|██████████| 921/921 [02:02<00:00,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set avg loss: 2.034964 Acc: 36274/58894: 61.59%\n",
            "test set avg loss top-5: 2.034964 Acc: 51116/58894: 86.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# Running main code loop\n",
        "train_log, test_log = {}, {}\n",
        "test_log['loss'] = {}\n",
        "test_log['1accuracy'] = {}\n",
        "test_log['5accuracy'] = {}\n",
        "#for train logging\n",
        "countTrainBatch = 0\n",
        "countTestBatch = 0\n",
        "countEpoch = 0\n",
        "writer = SummaryWriter(\"logs\")\n",
        "for epoch in range(hyperparams.epochs):\n",
        "    print(f'starting epoch {epoch}')\n",
        "    train_log = train(hyperparams, model, device, train_loader, optimizer, epoch, train_log, countTrainBatch, writer)\n",
        "    countTrainBatch += len(train_loader)\n",
        "    test_log = test(model, device, test_loader, epoch, test_log,countTestBatch,writer)\n",
        "    writer.add_scalar('Top-1 Test/Accuracy', test_log['1accuracy'][epoch], countEpoch)\n",
        "    writer.add_scalar('Top-5 Test/Accuracy', test_log['5accuracy'][epoch], countEpoch)\n",
        "    countTestBatch += len(test_loader)\n",
        "    countEpoch+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPS9U5EwRgl7"
      },
      "outputs": [],
      "source": [
        "# Save hyperparameters and model weights\n",
        "\n",
        "# You'll want to think about how to structure your\n",
        "# data drive so it doesn't get too messy from checkpoints\n",
        "# alternately, you can replace this with wandb if you go that route\n",
        "checkpoint_path = '/content/gdrive/Shareddrives/197/project_1'\n",
        "savename = datetime.datetime.now().strftime(\"%H%M_%m%d%Y\")\n",
        "# save your hyperparameters\n",
        "with open(f\"{checkpoint_path}hyperparams_{savename}.json\", 'w') as f:\n",
        "  json.dumps(vars(hyperparams))\n",
        "\n",
        "# save your model state\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch' : hyperparams.epochs,\n",
        "            'train_log' : train_log,\n",
        "            'test_log' : test_log,\n",
        "\n",
        "            }, f\"{checkpoint_path}model_{savename}.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM9ywUlvE1Mq"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8QD7qIyQMVO"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmgnIXZm93xp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9f4d9fc3-cf5f-49e9-9956-a8d4b2e8dd76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwriter = SummaryWriter(\"logs\")\\n\\nfor n_iter in range(14):\\n    writer.add_scalar(\\'Loss/train\\', train_log[n_iter], n_iter)\\n    writer.add_scalar(\\'Loss/test\\', test_log[\\'loss\\'][n_iter], n_iter)\\n    writer.add_scalar(\\'Accuracy/train\\', np.random.random(), n_iter)\\n    writer.add_scalar(\\'Accuracy/test\\', test_log[\\'accuracy\\'][n_iter], n_iter)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#Add train logging\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "writer = SummaryWriter(\"logs\")\n",
        "\n",
        "for n_iter in range(14):\n",
        "    writer.add_scalar('Loss/train', train_log[n_iter], n_iter)\n",
        "    writer.add_scalar('Loss/test', test_log['loss'][n_iter], n_iter)\n",
        "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
        "    writer.add_scalar('Accuracy/test', test_log['accuracy'][n_iter], n_iter)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}